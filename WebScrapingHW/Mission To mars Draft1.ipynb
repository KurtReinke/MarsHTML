{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyMongo to work with MongoDBs\n",
    "connection = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = bs(response.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database and collection\n",
    "db = client.mars_news_db\n",
    "collection = db.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Examine the results, then determine element that contains sought info\n",
    "# results are returned as an iterable list\n",
    "#Make sure scraping is working properly\n",
    "news_results = soup.find_all('li', class_='result-row')\n",
    "print(news_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News  – NASA’s Mars Exploration Program \n"
     ]
    }
   ],
   "source": [
    "#TItle scrape and mess around\n",
    "title1 = soup.title.text\n",
    "print(title1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape the NASA Mars News Site and collect the latest News Title and Paragraph Text. \n",
    "#Assign the text to variables that you can reference later.\n",
    "#-----------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The Launch Is Approaching for NASA's Next Mars Rover, Perseverance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NASA to Hold Mars 2020 Perseverance Rover Launch Briefing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Alabama High School Student Names NASA's Mars Helicopter\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mars Helicopter Attached to NASA's Perseverance Rover\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NASA's Perseverance Mars Rover Gets Its Wheels and Air Brakes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "10.9 Million Names Now Aboard NASA's Perseverance Mars Rover\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print all Heading texts\n",
    "headers =  soup.find_all('div', class_=\"content_title\")\n",
    "for header in headers:\n",
    "    print(header.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Red Planet's surface has been visited by eight NASA spacecraft. The ninth will be the first that includes a roundtrip ticket in its flight plan. \n",
      "\n",
      "\n",
      "Learn more about the agency's next Red Planet mission during a live event on June 17.\n",
      "\n",
      "\n",
      "Vaneeza Rupani's essay was chosen as the name for the small spacecraft, which will mark NASA's first attempt at powered flight on another planet.\n",
      "\n",
      "\n",
      "The team also fueled the rover's sky crane to get ready for this summer's history-making launch.\n",
      "\n",
      "\n",
      "After the rover was shipped from JPL to Kennedy Space Center, the team is getting closer to finalizing the spacecraft for launch later this summer.\n",
      "\n",
      "\n",
      "As part of NASA's 'Send Your Name to Mars' campaign, they've been stenciled onto three microchips along with essays from NASA's 'Name the Rover' contest. Next stop: Mars.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print all paragraph texts\n",
    "#Use inner to get rid of empyty space\n",
    "paragraphs = soup.find_all('div', class_=\"rollover_description_inner\")\n",
    "for paragraph in paragraphs:\n",
    "    print(paragraph.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Launch Is Approaching for NASA's Next Mars Rover, Perseverance\n"
     ]
    }
   ],
   "source": [
    "news_title = soup.find('div', class_ = 'content_title').text.strip()\n",
    "print(news_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Red Planet's surface has been visited by eight NASA spacecraft. The ninth will be the first that includes a roundtrip ticket in its flight plan.\n"
     ]
    }
   ],
   "source": [
    "#I dont understand why this first part doesn't work? it labels it as an article teaser body\n",
    "\n",
    "#n1 = soup.find('div', class_ = \"article_teaser_body\").text.strip()\n",
    "#>No matter where you live, choose from a menu of activities to join NASA as we \"Countdown to Mars\" and launch the Perseverance rover to the Red Planet.</div>\n",
    "news_p = soup.find('div', class_ = \"rollover_description\").text.strip()\n",
    "\n",
    "print(news_p)\n",
    "\n",
    "#And now it pulls the 5th article down the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through returned results: headers, paragraphs\n",
    "for news_result in news_results:\n",
    "        news_title = headers.find('div', class_=\"content_title\").text\n",
    "        news_p = paragraphs.find('div', class_=\"rollover_description_inner\").text\n",
    "\n",
    "        print(news_title)\n",
    "        print(news_p)\n",
    "        \n",
    "#Is this necessary?  Brings the info into Mongodb i believe after looping if I didnt goof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JPL Mars Space Images - Featured Image\n",
    "#Visit the url for JPL Featured Space Image here.\n",
    "\n",
    "#Use splinter to navigate the site and find the image url for the current \n",
    "#Featured Mars Image and assign the url string to a variable called featured_image_url.\n",
    "#Make sure to find the image url to the full size .jpg image.\n",
    "#Make sure to save a complete url string for this image.\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Vars\n",
    "url2=\"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "\n",
    "response2 = requests.get(url2)\n",
    "i_soup=bs(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#Find Images\n",
    "image = i_soup.find_all('div', class_=\"img\")\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Image URL\n",
    "img_url = []\n",
    "for image in images:\n",
    "    mars_image = image['data-fancybox-href']\n",
    "    img_url.append(mars_image)\n",
    "\n",
    "img_url2 = img_url + mars_image\n",
    "img_url2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
